{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### National Park Service Website Scraper & NLP Analysis\n",
    "### Code by Eric Englin\n",
    "<br><br>\n",
    "#### Objective: scrape all 395 websites for NPS to see if they meet the 10 essential travelers information for transportation\n",
    "#### These 10 are:\n",
    "<li>driving directions</li>\n",
    "<li>Public transportation information</li>\n",
    "<li>Bike and pedestrian information</li>\n",
    "<li>Parking lot locations and accommodations</li>\n",
    "<li>Parking lot peak use and availability</li>\n",
    "<li>congestion information</li>\n",
    "<li>travel distances and travel time to sites within the park</li>\n",
    "<li>Accessibility</li>\n",
    "<li>Description of transportation experience</li>\n",
    "<li>Alternative fueling stations</li>\n",
    "\n",
    "<br><br>\n",
    "#### Together, these 10 measures can allow NPS to understand how their parks are providing transportation to visitors. This information can be used to evaluate each park and plan for an improved park experience in the future. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib as mpl\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import time\n",
    "pd.set_option('display.width', 500)\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.notebook_repr_html', True)\n",
    "import seaborn as sns \n",
    "import html\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import os\n",
    "from selenium import webdriver\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import warnings; warnings.simplefilter('ignore')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#change to the location where you saved chromedriver\n",
    "#if issues here, make sure that we have correct chromedriver installed version:\n",
    "        # (check google chrome version -- It'll be somewhere between 73-79)\n",
    "chromedriver_location=r'C:\\Users\\eric.englin\\Downloads/chromedriver.exe'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome(executable_path=chromedriver_location) \n",
    "driver.get('https://www.nps.gov/AGFO/planyourvisit/directions.htm')\n",
    "driver.close() #close driver link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"Park Unit Scraping Information.csv\"\n",
    "parks = pd.read_csv(path, encoding='latin-1')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "393"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(parks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = []\n",
    "for x in parks['Alpha']:\n",
    "    y = \"https://www.nps.gov/\"+x+\"/index.htm\"\n",
    "    index.append(y)\n",
    "\n",
    "parks['index site']=index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.nps.gov/BEOL/index.htm\n",
      "https://www.nps.gov/CAHA/index.htm\n",
      "https://www.nps.gov/CHPI/index.htm\n",
      "https://www.nps.gov/DEWA/index.htm\n",
      "https://www.nps.gov/FOLS/index.htm\n",
      "https://www.nps.gov/GATE/index.htm\n",
      "https://www.nps.gov/GWMP/index.htm\n",
      "https://www.nps.gov/JELA/index.htm\n",
      "https://www.nps.gov/LAVO/index.htm\n",
      "https://www.nps.gov/MLKM/index.htm\n",
      "https://www.nps.gov/NWWM/index.htm\n",
      "https://www.nps.gov/POPO/index.htm\n",
      "https://www.nps.gov/SAJU/index.htm\n",
      "https://www.nps.gov/THRO/index.htm\n",
      "https://www.nps.gov/WEFA/index.htm\n"
     ]
    }
   ],
   "source": [
    "## For context, here is the main site for a sample of national parks\n",
    "y=0\n",
    "for x in parks['index site']:\n",
    "    y+=1\n",
    "    if y%25 ==0:\n",
    "        print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_site(park):\n",
    "    from selenium.common.exceptions import InvalidArgumentException\n",
    "\n",
    "    driver = webdriver.Chrome(executable_path=chromedriver_location) #change location\n",
    "    link = \"https://www.nps.gov/\"+park+\"/index.htm\"\n",
    "    driver.get(link)\n",
    "    #driver.find_element_by_xpath('//*[@id=\"anch_15\"]').click()\n",
    "    page_source = driver.page_source\n",
    "    soup = BeautifulSoup(page_source, 'lxml')\n",
    "    website_list=[]\n",
    "    raw_list=[]\n",
    "    website_content=[]\n",
    "    for l in soup.find_all('a'):\n",
    "        try:\n",
    "            if \"planyourvisit\" in l.get('href') and l.get('href') not in raw_list: #only want plan your visit sites\n",
    "                if \"https://www.nps.gov\" in l.get('href'):\n",
    "                    z= l.get('href')\n",
    "                    raw_list.append(z)\n",
    "                    website_list.append(z)\n",
    "                else:\n",
    "                    z = l.get('href')\n",
    "                    raw_list.append(z)\n",
    "                    z = \"https://www.nps.gov\"+z\n",
    "                    website_list.append(z)\n",
    "        except:\n",
    "            pass\n",
    "    for x in website_list:\n",
    "        try:\n",
    "            driver.get(x)\n",
    "            page_source = driver.page_source\n",
    "            soup = BeautifulSoup(page_source, 'lxml')\n",
    "            raw_content = soup.get_text(strip=True) #all text fields are scraped \n",
    "            website_content.append(raw_content) #raw content added to list of all content\n",
    "        except:\n",
    "            #This means that the webpage doesn't exist\n",
    "            pass\n",
    "    driver.close() #close driver link at end of scrape\n",
    "    dict = {'website page': website_list, 'content': website_content}  #create dataframe for park data\n",
    "    park_data = pd.DataFrame(dict) \n",
    "    park_data['park']=park\n",
    "    return park_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = [\"i\", \"me\", \"my\", \"myself\", \"we\", \"our\", \"ours\", \"ourselves\", \n",
    "             \"you\", \"your\", \"yours\", \"yourself\", \"yourselves\", \"he\", \"him\", \n",
    "             \"his\", \"himself\", \"she\", \"her\", \"hers\", \"herself\", \"it\", \"its\", \"itself\", \n",
    "             \"they\", \"them\", \"their\", \"theirs\", \"themselves\", \"what\", \"which\", \"who\",\n",
    "             \"whom\", \"this\", \"that\", \"these\", \"those\", \"am\", \"is\", \"are\", \"was\", \n",
    "             \"were\", \"be\", \"been\", \"being\", \"have\", \"has\", \"had\", \"having\", \"do\", \n",
    "             \"does\", \"did\", \"doing\", \"a\", \"an\", \"the\", \"and\", \"but\", \"if\", \"or\", \n",
    "             \"because\", \"as\", \"until\", \"while\", \"of\", \"at\", \"by\", \"for\", \"with\", \"about\",\n",
    "             \"against\", \"between\", \"into\", \"through\", \"during\", \"before\", \"after\", \"above\", \n",
    "             \"below\", \"to\", \"from\", \"up\", \"down\", \"in\", \"out\", \"on\", \"off\", \"over\", \"under\", \n",
    "             \"again\", \"further\", \"then\", \"once\", \"here\", \"there\", \"when\", \"where\", \"why\", \n",
    "             \"how\", \"all\", \"any\", \"both\", \"each\", \"few\", \"more\", \"most\", \"other\", \"some\", \n",
    "             \"such\", \"no\", \"nor\", \"not\", \"only\", \"own\", \"same\", \"so\", \"than\", \"too\", \"very\", \n",
    "             \"s\", \"t\", \"can\", \"will\", \"just\", \"don\", \"should\", \"now\",\n",
    "            \"href\",\"=\",\"/\",\">\",\"<\",\"]\",\"[\",\"span\",\"'\\n'\",'class',\"jstcache\",\n",
    "            \"onclick\",\"null\",\"jscontent\",\" <br/>\",\"</span>\",\",\",\";\",\"(\",\")\",\"{\",\"}\",\":\",\"''\",\n",
    "            \"&\",\"'\",\"var\",\"+=\",\".\",\"#\",\"-\",\"=\",\"+\",\"``\",\"0\",\"â€™\",\"data.operatingHours\",\"outputVarOperatingHours\",\n",
    "            \".exceptions\",\"--\",\"1\",\"-1\",\"?\",\"class=\",\"==\",\"div\",\"/div\",\"$\",\"li\",\"e\",\"!\",\"k\",\"/span\",\"jQuery\",\n",
    "            \"tabindex\",'j','l']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Traveler_Info_Finder(park):\n",
    "    \"\"\"\n",
    "    Find the following fields:\n",
    "    #Public transportation information\n",
    "    #Alternative Fueling Stations\n",
    "    #Bike/Pedestrian Information\n",
    "    #Driving directions\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    AFS_list = []\n",
    "    Bike_Ped_count = []\n",
    "    Directions_count = []\n",
    "    Directions_page_count = []\n",
    "    Pub_Transit_count = []\n",
    "    Direction_majorcount = []\n",
    "    Direction_count = []\n",
    "    Congestion_count = []\n",
    "    Travel_dist_count = []\n",
    "    Travel_dist_other_count=[]\n",
    "    Accessibility_count=[]\n",
    "    Parking_count=[]\n",
    "    Parking_plan_count=[]\n",
    "    \n",
    "\n",
    "    Directions_Words = [\"Entrance\",\"Center\",\"street\",\"Visitor\"\n",
    "                        \"Street\",\"parking\",\"directions\",\"Route\",\"Road\",\n",
    "                        \"Interstate\",\"Exit\",\n",
    "                        \"mile\",\"km\",\"ferry\",\"access\", \"Street\",\"Blvd\", \"Hwy\"\n",
    "                       ]\n",
    "\n",
    "    Directions_MajorWords = [\n",
    "        \"GPS Coordinates\", \"GPS coordinates\", \"GPS device address\", \"GPS address\",\n",
    "        \"Latitude\",\"Longitude\",\"Street\",\n",
    "        \"Blvd\", \"Boulevard\", \"Ln.\",\"Rd.\",\"Pl.\",\n",
    "        \"Hwy\",\"Exit\",\"Interstate\",\"US Highway\", \"U.S. Highway\", \"Indian Head Highway\",  \"Turnpike\",\"beltway\",\"Causeway\"\n",
    "        \"Secondary Route\", \"State Route\", \"I-\",\"State Highway\"\n",
    "        ]\n",
    "\n",
    "\n",
    "    Public_Transit_Words = [\n",
    "        \"Public Transportation\", \"public transportation\",\"Public transportation\",\n",
    "        \"bus schedule\", \"Bus schedule\", \"shuttle\", \"shuttles\",\"Shuttle\",\n",
    "        \"bus stops\", \"buses stop\", \"ferry\",\"transit\",\"Transit\"\n",
    "    ]\n",
    "\n",
    "    Congestion_Words = [\n",
    "       \"congestion\",\"Congestion\", \"congested\"\n",
    "    ]\n",
    "    \n",
    "    BicyclePed_Words = [\n",
    "        \"Bicyclists\",\"bicyclists\",\"cyclists\",\"pedestrians\",\"biking\"\n",
    "        #,\"biking\",\"Biking\"\n",
    "    ]\n",
    "    \n",
    "    Travel_dist_Words = [\n",
    "        'miles'\n",
    "    ]\n",
    "\n",
    "    Travel_dist_other_Words = [\n",
    "        'Places To Go',\"Popular Destinations\"\n",
    "    ]\n",
    "    \n",
    "    Accessibility_Words = [\n",
    "        \"wheelchair\", \"accessibility\", \"disability\", \"impaired\", \"disabilities\", \"handicap\",\n",
    "        \"accessible\",\"Wheelchair\"\n",
    "    ]\n",
    "    \n",
    "    Parking_Words = [\n",
    "        \"parking\", \"Parking\", \"pullout\"\n",
    "    ]\n",
    "\n",
    "    \n",
    "    #this will get the number of sites that have keywords\n",
    "    count=0\n",
    "    for x in park[\"content\"]:\n",
    "        try:\n",
    "            y=0\n",
    "            if \"Department of Energy\" in x and \"Alternative Fueling Station\" in x:\n",
    "                y=1\n",
    "                AFS_list.append(y)\n",
    "            else:\n",
    "                y=0\n",
    "                AFS_list.append(y)\n",
    "            if any(substring in x for substring in Public_Transit_Words):\n",
    "                y=1\n",
    "                Pub_Transit_count.append(y)\n",
    "            else:\n",
    "                y=0\n",
    "                Pub_Transit_count.append(y)\n",
    "            if any(substring in x for substring in Directions_MajorWords):\n",
    "                y=1\n",
    "                Direction_majorcount.append(y)\n",
    "            else:\n",
    "                y=0\n",
    "                Direction_majorcount.append(y)\n",
    "            if any(substring in x for substring in BicyclePed_Words):\n",
    "                y=1\n",
    "                Bike_Ped_count.append(y)\n",
    "            else:\n",
    "                y=0\n",
    "                Bike_Ped_count.append(y)\n",
    "            if any(substring in x for substring in Congestion_Words):\n",
    "                y=1\n",
    "                Congestion_count.append(y)\n",
    "            else:\n",
    "                y=0\n",
    "                Congestion_count.append(y)\n",
    "            if any(substring in x for substring in Travel_dist_Words):\n",
    "                y=1\n",
    "                Travel_dist_count.append(y)\n",
    "            else:\n",
    "                y=0\n",
    "                Travel_dist_count.append(y)\n",
    "            if any(substring in x for substring in Travel_dist_other_Words):\n",
    "                y=1\n",
    "                Travel_dist_other_count.append(y)\n",
    "            else:\n",
    "                y=0\n",
    "                Travel_dist_other_count.append(y)\n",
    "            if any(substring in x for substring in Parking_Words):\n",
    "                y=1\n",
    "                Parking_count.append(y)\n",
    "            else:\n",
    "                y=0\n",
    "                Parking_count.append(y)           \n",
    "            if any(substring in x for substring in Directions_Words):\n",
    "                y=1\n",
    "                Directions_page_count.append(y)\n",
    "            else:\n",
    "                y=0\n",
    "                Directions_page_count.append(y)           \n",
    "        except:\n",
    "            y=0\n",
    "            AFS_list.append(y)\n",
    "            Pub_Transit_count.append(y)\n",
    "            Direction_majorcount.append(y)\n",
    "            Bike_Ped_count.append(y)\n",
    "            Congestion_count.append(y)\n",
    "            Travel_dist_count.append(y)\n",
    "            Travel_dist_other_count.append(y)\n",
    "            Parking_count.append(y)\n",
    "            Directions_page_count.append(y)\n",
    "\n",
    "                \n",
    "\n",
    "                \n",
    "#this section will get the total number of times that keywords show up on all sites for a park\n",
    "    ps = PorterStemmer()\n",
    "    lem = WordNetLemmatizer()\n",
    "\n",
    "    stemmed_words=[]\n",
    "\n",
    "    for x in park['content']:\n",
    "        z=0\n",
    "        z2=0\n",
    "        z3=0\n",
    "        major = 0\n",
    "        congestion = 0\n",
    "        pubtrans=0\n",
    "        bikeped=0\n",
    "        try:\n",
    "            tokenized_word=word_tokenize(x)\n",
    "            filtered_sent=[]\n",
    "            stemmed_words=[]\n",
    "            direction_words_temp = []\n",
    "            for w in tokenized_word:\n",
    "                if w not in stopwords:\n",
    "                    filtered_sent.append(w)\n",
    "            for w in filtered_sent:\n",
    "                if w in Directions_Words:\n",
    "                    z += 1\n",
    "                if w in Parking_Words:\n",
    "                    z2 +=1\n",
    "                if w in Accessibility_Words:\n",
    "                    z3 += 1\n",
    "            Direction_count.append(z)\n",
    "            Parking_plan_count.append(z2)\n",
    "            Accessibility_count.append(z3)\n",
    "        except:\n",
    "            Direction_count.append(0)\n",
    "            Parking_plan_count.append(0)\n",
    "            Accessibility_count.append(0)\n",
    "            \n",
    "    park[\"Alternative_Fueling_Stations\"]=AFS_list\n",
    "    park[\"MajorDirections_count\"]=Direction_majorcount\n",
    "    park[\"Directions_count\"]=Direction_count\n",
    "    park[\"Directions_page_count\"]=Directions_page_count\n",
    "    park[\"Public_transportation_information\"]=Pub_Transit_count\n",
    "    park[\"Congestion_information\"]=Congestion_count\n",
    "    park[\"Bike_Pedestrian_Information\"]=Bike_Ped_count\n",
    "    park[\"Travel_dist_information\"]=Travel_dist_count\n",
    "    park[\"Travel_other_dist_information\"]=Travel_dist_other_count\n",
    "    park['Accessibility_intro_information']=Accessibility_count\n",
    "    park['Parking_information']=Parking_count\n",
    "    park['Parking_experience_information']=Parking_plan_count\n",
    "    park['Parking_max_on_one_site']=park['Parking_experience_information']\n",
    "    \n",
    "\n",
    "    park['Accessibility_information']=np.where(\n",
    "        np.logical_or(park['Accessibility_intro_information']>2, \n",
    "                     park['Parking_experience_information']>2),1,0)\n",
    "\n",
    "    park_final = park.groupby('park', as_index=False).agg({\n",
    "        \"MajorDirections_count\": \"sum\",\n",
    "        \"Directions_count\": \"sum\",\n",
    "        \"Directions_page_count\":\"sum\",\n",
    "        \"Public_transportation_information\": \"sum\",\n",
    "        \"Alternative_Fueling_Stations\":\"sum\",\n",
    "        \"Bike_Pedestrian_Information\":\"sum\",\n",
    "        'Congestion_information':'sum',\n",
    "        'Travel_dist_information':'sum',\n",
    "        'Travel_other_dist_information':'sum',\n",
    "        'Accessibility_information':'sum',\n",
    "        'Parking_information':'sum',\n",
    "        'Parking_experience_information':'sum',\n",
    "        'Parking_max_on_one_site':'max',\n",
    "        \"website page\":\"count\",\n",
    "    })\n",
    "    \n",
    "   # park_final2 = park.groupby('park')['Directions_word_list'].apply(lambda x: ','.join(x))\n",
    "   # park_final = park_final.merge(park_final2, on=\"park\")\n",
    "\n",
    "    return park_final\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEOL\n",
      "724\n",
      "CAHA\n",
      "1660\n"
     ]
    }
   ],
   "source": [
    "z=0\n",
    "v=0\n",
    "for x in parks['Alpha'].unique():\n",
    "    v+=1\n",
    "    if z==0:\n",
    "        park_scrape_dataset=scrape_site(x)\n",
    "    else:\n",
    "        this_park_scrape = scrape_site(x)\n",
    "        park_scrape_dataset = park_scrape_dataset.append(this_park_scrape)\n",
    "    if v % 25 == 0:\n",
    "        print(x)\n",
    "        print(len(park_scrape_dataset))\n",
    "    z+=1\n",
    "    if z>50: #if want to test out\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save your scraped website into another variable name so don't have to redo scrape\n",
    "park_scrape_dataset2 = park_scrape_dataset\n",
    "\n",
    "#data cleaning\n",
    "park_scrape_dataset2['index1'] = park_scrape_dataset2.index\n",
    "park_scrape_dataset2=park_scrape_dataset2.reset_index()\n",
    "\n",
    "\n",
    "#save as excel\n",
    "#note: saving as a csv won't work due to punctuation used in html code\n",
    "park_scrape_dataset2.to_excel(\"full_park_scrape_dataset2.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>website page</th>\n",
       "      <th>content</th>\n",
       "      <th>park</th>\n",
       "      <th>index1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1689</th>\n",
       "      <td>29</td>\n",
       "      <td>https://www.nps.gov/planyourvisit/index.htm</td>\n",
       "      <td>Plan Your Visit (U.S. National Park Service)va...</td>\n",
       "      <td>CAKR</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1690</th>\n",
       "      <td>30</td>\n",
       "      <td>https://www.nps.gov/planyourvisit/event-search...</td>\n",
       "      <td>Event Calendar (U.S. National Park Service)var...</td>\n",
       "      <td>CAKR</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1691</th>\n",
       "      <td>31</td>\n",
       "      <td>https://www.nps.gov/planyourvisit/passes.htm</td>\n",
       "      <td>America the Beautiful Passes (U.S. National Pa...</td>\n",
       "      <td>CAKR</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1692</th>\n",
       "      <td>32</td>\n",
       "      <td>https://www.nps.gov/planyourvisit/trip-ideas.htm</td>\n",
       "      <td>Trip Ideas (U.S. National Park Service)var jsD...</td>\n",
       "      <td>CAKR</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1693</th>\n",
       "      <td>33</td>\n",
       "      <td>https://www.nps.gov/cakr/planyourvisit/permits...</td>\n",
       "      <td>NPS - Page In-ProgressPage In-ProgressThis pag...</td>\n",
       "      <td>CAKR</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      index                                       website page                                            content  park  index1\n",
       "1689     29        https://www.nps.gov/planyourvisit/index.htm  Plan Your Visit (U.S. National Park Service)va...  CAKR      29\n",
       "1690     30  https://www.nps.gov/planyourvisit/event-search...  Event Calendar (U.S. National Park Service)var...  CAKR      30\n",
       "1691     31       https://www.nps.gov/planyourvisit/passes.htm  America the Beautiful Passes (U.S. National Pa...  CAKR      31\n",
       "1692     32   https://www.nps.gov/planyourvisit/trip-ideas.htm  Trip Ideas (U.S. National Park Service)var jsD...  CAKR      32\n",
       "1693     33  https://www.nps.gov/cakr/planyourvisit/permits...  NPS - Page In-ProgressPage In-ProgressThis pag...  CAKR      33"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "park_scrape_dataset2.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Park:  BEOL :  25  checks done;  375  remaining; Processing Time:  30.670907299999726\n",
      "Current Park:  CAHA :  50  checks done;  350  remaining; Processing Time:  33.50025940000023\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Model to calculate VE fields ##\n",
    "\n",
    "#create new sheet so with our variables for each park\n",
    "park_sheet = pd.DataFrame(columns = ['park', 'Driving_Directions','Public_transportation_information',\n",
    "                                     'Bike_Pedestrian_Information','Congestion_information','Accessibility',\n",
    "                                           'Alternative_Fueling_Stations', 'website page count'])\n",
    "z=0\n",
    "tic = time.clock() #function to let us track processing time\n",
    "\n",
    "\n",
    "for x in park_scrape_dataset2['park'].unique():\n",
    "    z+=1\n",
    "    if z % 25 == 0: \n",
    "        #function to let us track processing time\n",
    "        z5 = 400-z\n",
    "        toc = time.clock()\n",
    "        time_diff = toc-tic\n",
    "        print(\"Current Park: \", x, \": \", z, \" checks done; \", z5, \" remaining; Processing Time: \",time_diff)\n",
    "        tic=toc\n",
    "        \n",
    "    this_park = park_scrape_dataset[(park_scrape_dataset['park']==x)] #filter our webscraping dataset for our park's website code\n",
    "    park_final = Traveler_Info_Finder(this_park) #run function\n",
    "    park_sheet = park_sheet.append({'park': park_final.get_value(0,'park'),\n",
    "                        'website page count': park_final.get_value(0,'website page'),\n",
    "                        'Directions_word_count': park_final.get_value(0,'Directions_count'),\n",
    "                        'Directions_page_count': park_final.get_value(0,'Directions_page_count'),\n",
    "                       'Driving_Directions': park_final.get_value(0,'MajorDirections_count'),\n",
    "                       'Public_transportation_information': park_final.get_value(0,'Public_transportation_information'),\n",
    "                       'Alternative_Fueling_Stations': park_final.get_value(0,'Alternative_Fueling_Stations'), \n",
    "                       'Bike_Pedestrian_Information': park_final.get_value(0,'Bike_Pedestrian_Information'),\n",
    "                       'Congestion_information': park_final.get_value(0,'Congestion_information'),\n",
    "                        'Travel_Distance_Information': park_final.get_value(0,'Travel_dist_information'),\n",
    "                        'Travel_other_dist_information': park_final.get_value(0,'Travel_other_dist_information'),\n",
    "                        'Accessibility': park_final.get_value(0,'Accessibility_information'),\n",
    "                        'Parking_raw_information': park_final.get_value(0,'Parking_information'),\n",
    "                        'Parking_experience_information': park_final.get_value(0,'Parking_experience_information'),\n",
    "                        'Parking_max_on_one_site': park_final.get_value(0,'Parking_max_on_one_site')\n",
    "                       },\n",
    "                      ignore_index=True)\n",
    "    park_sheet.loc[park_sheet.Driving_Directions > 0, 'Driving_Directions'] = 1\n",
    "    park_sheet.loc[park_sheet.Alternative_Fueling_Stations > 0, 'Alternative_Fueling_Stations'] = 1\n",
    "    park_sheet.loc[park_sheet.Public_transportation_information > 0, 'Public_transportation_information'] = 1\n",
    "    park_sheet.loc[park_sheet.Bike_Pedestrian_Information > 0, 'Bike_Pedestrian_Information'] = 1\n",
    "    park_sheet.loc[park_sheet.Congestion_information > 0, 'Congestion_information'] = 1\n",
    "    park_sheet.loc[park_sheet.Accessibility > 0, 'Accessibility'] = 1\n",
    " #   park_sheet.loc[park_sheet.Parking_information > 0, 'Parking_information'] = 1\n",
    "    park_sheet['Travel_Distance_Final']=np.where(\n",
    "        np.logical_or(park_sheet['Travel_Distance_Information']>9, \n",
    "                     park_sheet['Travel_other_dist_information']>0),1,0)\n",
    "    park_sheet['Parking_Experience_information']=np.where((\n",
    "        park_sheet['Parking_raw_information']/park_sheet['website page count'])>0.25,1,0)\n",
    "    park_sheet['Transportation_experience_information']=np.where((\n",
    "        park_sheet['Directions_page_count']/park_sheet['website page count'])>0.65,1,0)\n",
    "    park_sheet['Parking_information']=np.where(np.logical_or(\n",
    "        park_sheet['Parking_Experience_information']==1,\n",
    "        park_sheet['Parking_max_on_one_site']>2),1,0)\n",
    "\n",
    "\n",
    "park_sheet= park_sheet.drop(columns=['website page count', 'Directions_word_count',\n",
    "                        'Directions_page_count','Parking_raw_information','Parking_experience_information',\n",
    "                        'Parking_max_on_one_site','Travel_Distance_Information','Travel_other_dist_information'])\n",
    "    \n",
    "#create csv\n",
    "park_sheet.to_csv(\"final_park2.csv\") #save final csv\n",
    "os.system(\"start EXCEL.EXE final_park2.csv\") #open csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acadia National Park can be found at lat:44.30777545, long:-68.30063316.\n"
     ]
    }
   ],
   "source": [
    "## In progress cell ##\n",
    "## Could use lat/longs to immediately make a map to visualize the output data\n",
    "\n",
    "import urllib.request, json\n",
    "\n",
    "# Configure API request\n",
    "park = \"acad\"\n",
    "endpoint = \"https://developer.nps.gov/api/v1/parks?parkCode=\" + park + \"&api_key=g6HPfqqNODFIg64arFX7CPrRwSxoFci42Wu54IZ7\"\n",
    "HEADERS = {\"Authorization\":\"g6HPfqqNODFIg64arFX7CPrRwSxoFci42Wu54IZ7\"}\n",
    "req = urllib.request.Request(endpoint,headers=HEADERS)\n",
    "\n",
    "# Execute request and parse response\n",
    "response = urllib.request.urlopen(req).read()\n",
    "data = json.loads(response.decode('utf-8'))\n",
    "\n",
    "# Prepare and execute output\n",
    "print(data[\"data\"][0][\"fullName\"] + \" can be found at \" + data[\"data\"][0][\"latLong\"] + \".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "park_data = pd.read_excel(\"full_park_scrape_dataset2.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>index</th>\n",
       "      <th>website page</th>\n",
       "      <th>content</th>\n",
       "      <th>park</th>\n",
       "      <th>index1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>https://www.nps.gov/abli/planyourvisit/index.htm</td>\n",
       "      <td>Plan Your Visit - Abraham Lincoln Birthplace N...</td>\n",
       "      <td>ABLI</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>https://www.nps.gov/abli/planyourvisit/basicin...</td>\n",
       "      <td>Basic Information - Abraham Lincoln Birthplace...</td>\n",
       "      <td>ABLI</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>https://www.nps.gov/abli/planyourvisit/hours.htm</td>\n",
       "      <td>Operating Hours &amp; Seasons for 2019 - Abraham L...</td>\n",
       "      <td>ABLI</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>https://www.nps.gov/abli/planyourvisit/fees.htm</td>\n",
       "      <td>Fees &amp; Passes - Abraham Lincoln Birthplace Nat...</td>\n",
       "      <td>ABLI</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>https://www.nps.gov/abli/planyourvisit/conditi...</td>\n",
       "      <td>Alerts &amp; Conditions - Abraham Lincoln Birthpla...</td>\n",
       "      <td>ABLI</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  index                                       website page                                            content  park  index1\n",
       "0           0      0   https://www.nps.gov/abli/planyourvisit/index.htm  Plan Your Visit - Abraham Lincoln Birthplace N...  ABLI       0\n",
       "1           1      1  https://www.nps.gov/abli/planyourvisit/basicin...  Basic Information - Abraham Lincoln Birthplace...  ABLI       1\n",
       "2           2      2   https://www.nps.gov/abli/planyourvisit/hours.htm  Operating Hours & Seasons for 2019 - Abraham L...  ABLI       2\n",
       "3           3      3    https://www.nps.gov/abli/planyourvisit/fees.htm  Fees & Passes - Abraham Lincoln Birthplace Nat...  ABLI       3\n",
       "4           4      4  https://www.nps.gov/abli/planyourvisit/conditi...  Alerts & Conditions - Abraham Lincoln Birthpla...  ABLI       4"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "park_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# About 7% of all sites were down at the time of data pull\n",
    "error_flag = []\n",
    "for x in park_scrape_dataset2[\"content\"]:\n",
    "    y=0\n",
    "    try:\n",
    "        if \"failed for element\" in x:\n",
    "            y=1\n",
    "            error_flag.append(y)\n",
    "        else:\n",
    "            y=0\n",
    "            error_flag.append(y)\n",
    "    except:\n",
    "        y=0\n",
    "        error_flag.append(y)\n",
    "            \n",
    "park_scrape_dataset2['error_flag']=error_flag\n",
    "bad_website_page = park_scrape_dataset2.loc[park_scrape_dataset2.error_flag > 0]\n",
    "bad_website_page.to_csv(\"Error Website List.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_website_page.to_csv(\"Error Website List.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
